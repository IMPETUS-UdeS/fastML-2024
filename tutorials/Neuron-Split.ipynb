{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a32edca4-4875-42a1-a4b5-818504e2d976",
   "metadata": {},
   "source": [
    "# Manual Neuron Split tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3f3f51",
   "metadata": {},
   "source": [
    "### Vivado container bug fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae185577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['LD_PRELOAD'] = '/usr/lib/x86_64-linux-gnu/libudev.so.1'\n",
    "os.environ['PATH'] = os.environ['XILINX_VIVADO'] + '/bin:' + os.environ['PATH']\n",
    "#os.environ['LM_LICENSE_FILE'] = 'XXXX@your.xilinx.licence.server' or filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed59406f",
   "metadata": {},
   "source": [
    "## MNIST Example Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff53063",
   "metadata": {},
   "source": [
    "### Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790a7691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "# One-hot encode the labels\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Flatten the images and/or resize\n",
    "train_images = np.array([np.array(Image.fromarray(img).resize((16, 16))).flatten() for img in train_images])\n",
    "test_images = np.array([np.array(Image.fromarray(img).resize((16, 16))).flatten() for img in test_images])\n",
    "\n",
    "# Normalize the images to the range [0, 1]\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "np.save('X_train_val.npy', train_images)\n",
    "np.save('X_test.npy', test_images)\n",
    "np.save('y_train_val.npy', train_labels)\n",
    "np.save('y_test.npy', test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a93656",
   "metadata": {},
   "source": [
    "### Create and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707d191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Create & train model\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Dense(\n",
    "        46,\n",
    "        input_shape=(256,),\n",
    "        name='fc1',\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    Dense(\n",
    "        10,\n",
    "        name='output',\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "    )\n",
    ")\n",
    "model.add(Activation(activation='softmax', name='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='categorical_crossentropy',  # Use 'sparse_categorical_crossentropy' if labels are integers\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "    \n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88299cc",
   "metadata": {},
   "source": [
    "## Manual Neuron Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2987980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hls4ml\n",
    "import keras\n",
    "\n",
    "# list of sub models\n",
    "sub_models = []\n",
    "\n",
    "# Splitting layer 1 in 2 parallel sub_models\n",
    "# Get the original layer weights and biases\n",
    "original_layer = model.get_layer('fc1')\n",
    "weights, biases = original_layer.get_weights()\n",
    "\n",
    "# Split the weights and biases\n",
    "split_index = weights.shape[1] // 2\n",
    "weights1_1 = weights[:, :split_index]\n",
    "weights1_2 = weights[:, split_index:] \n",
    "biases1_1 = biases[:split_index]\n",
    "biases1_2 = biases[split_index:]\n",
    "\n",
    "# Create a new sub_model for the first half\n",
    "sub_model1 = Sequential()\n",
    "sub_model1.add(\n",
    "    Dense(\n",
    "            23,\n",
    "            input_shape=(256,),\n",
    "            name='fc1_1',\n",
    "            kernel_initializer='lecun_uniform',\n",
    "            kernel_regularizer=l1(0.0001)\n",
    "        )\n",
    ")\n",
    "# Set the weights\n",
    "sub_model1.layers[0].set_weights([weights1_1, biases1_1])\n",
    "sub_models.append(sub_model1)\n",
    "\n",
    "\n",
    "# Create a new sub_model for the second half\n",
    "sub_model2 = Sequential()\n",
    "sub_model2.add(\n",
    "    Dense(\n",
    "            23,\n",
    "            input_shape=(256,),\n",
    "            name='fc1_2',\n",
    "            kernel_initializer='lecun_uniform',\n",
    "            kernel_regularizer=l1(0.0001)\n",
    "        )\n",
    ")\n",
    "# set the second half\n",
    "sub_model2.layers[0].set_weights([weights1_2, biases1_2])\n",
    "sub_models.append(sub_model2)\n",
    "\n",
    "# Create a new sub_model with the remaining layer\n",
    "sub_model3 = keras.Sequential([keras.layers.InputLayer(model.layers[1].input_shape[1:]), model.layers[1], model.layers[2]])\n",
    "sub_models.append(sub_model3)\n",
    "\n",
    "sub_model1.build()\n",
    "sub_model2.build()\n",
    "sub_model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Evaluate the sub_models\n",
    "x1 = sub_model1.predict(test_images)\n",
    "x2 = sub_model2.predict(test_images)\n",
    "X_test = np.concatenate((x1, x2), axis=1)\n",
    "sub_model3.evaluate(X_test, test_labels, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed02fab0-131a-43c6-b41a-71bf6c563d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "4be45546577efffbeff6b841617f159c5d3aceed70e5e111773a23765fa7450f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
